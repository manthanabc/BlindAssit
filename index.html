<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Occulant | Vision Redefined</title>
    <meta name="description"
        content="State of the art system for visual navigation making use of AI, VLMs, Custom YOLOv11seg, and Haptic feedback.">
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=JetBrains+Mono:wght@100;400&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div id="noise"></div>

    <nav class="navbar">
        <div class="logo">OCCULANT<span>.sys</span></div>
        <div class="nav-links">
            <a href="#system">System</a>
            <a href="#architecture">Architecture</a>
            <a href="#demo" class="demo-link">Live Demo <span class="arrow">‚Üó</span></a>
        </div>
    </nav>

    <main class="hero">
        <div class="hero-content">
            <h1>Perception.<br><span class="highlight">Synthesized.</span></h1>
            <p>A completely autonomous visual navigation matrix. Powered by bespoke YOLOv11seg models, robust
                Vision-Language Models (VLMs), and real-time haptic translation.</p>
            <div class="cta-group">
                <a href="#demo" class="primary-btn">Initialize Live Demo</a>
                <a href="#architecture" class="secondary-btn">Read the Paper</a>
            </div>
        </div>

        <div class="hero-visual">
            <div class="visualizer-container">
                <canvas id="vision-canvas"></canvas>
                <div class="ui-overlay">
                    <div class="top-left corner"></div>
                    <div class="top-right corner"></div>
                    <div class="bottom-left corner"></div>
                    <div class="bottom-right corner"></div>

                    <div class="status-panel">
                        <div class="status-item">
                            <div class="dot blinking"></div> VLM: ACTIVE
                        </div>
                        <div class="status-item">
                            <div class="dot blinking"></div> YOLOv11-SEG: 120 FPS
                        </div>
                        <div class="status-item">
                            <div class="dot blinking"></div> HAPTIC: LINKED
                        </div>
                    </div>

                    <div id="terminal-readout" class="terminal-readout">
                        > Booting sequence...
                    </div>
                </div>
            </div>
        </div>
    </main>

    <section id="architecture" class="system-section">
        <div class="section-header">
            <h2>System Architecture</h2>
            <p>Information flows seamlessly from photon to haptic response in under 20 milliseconds.</p>
        </div>

        <div class="arch-container">
            <svg width="100%" viewBox="0 0 800 600" class="animated-arch">
                <defs>
                    <marker id="arrowhead" markerWidth="6" markerHeight="8" refX="5" refY="4" orient="auto">
                        <polygon points="0 0, 6 4, 0 8" fill="#fff" />
                    </marker>
                </defs>

                <!-- Nodes -->
                <g class="node n-video" transform="translate(100, 50)">
                    <rect width="200" height="100" rx="15" fill="rgba(0,0,0,0.5)" stroke="var(--border-color)"
                        stroke-width="2" />
                    <text x="100" y="52" fill="var(--text-primary)" text-anchor="middle" dominant-baseline="middle"
                        font-family="var(--font-sans)" font-size="18">VIDEO</text>
                </g>

                <g class="node n-llm-top" transform="translate(500, 50)">
                    <rect width="200" height="100" rx="15" fill="rgba(0,0,0,0.5)" stroke="var(--border-color)"
                        stroke-width="2" />
                    <text x="100" y="52" fill="var(--text-primary)" text-anchor="middle" dominant-baseline="middle"
                        font-family="var(--font-sans)" font-size="18">LLM</text>
                </g>

                <g class="node n-va" transform="translate(100, 250)">
                    <rect width="200" height="100" rx="15" fill="rgba(255,255,255,0.05)" stroke="var(--text-primary)"
                        stroke-width="2" />
                    <text x="100" y="52" fill="var(--text-primary)" text-anchor="middle" dominant-baseline="middle"
                        font-family="var(--font-sans)" font-size="18" font-weight="600">VisionAssist</text>
                </g>

                <g class="node n-ml" transform="translate(500, 250)">
                    <rect width="200" height="100" rx="15" fill="rgba(255,255,255,0.05)" stroke="var(--text-primary)"
                        stroke-width="2" />
                    <text x="100" y="52" fill="var(--text-primary)" text-anchor="middle" dominant-baseline="middle"
                        font-family="var(--font-sans)" font-size="18" font-weight="600">ML Model</text>
                </g>

                <g class="node n-llm-bot" transform="translate(430, 430)">
                    <rect width="80" height="40" rx="10" fill="rgba(0,0,0,0.5)" stroke="var(--border-color)"
                        stroke-width="2" />
                    <text x="40" y="22" fill="var(--text-primary)" text-anchor="middle" dominant-baseline="middle"
                        font-family="var(--font-sans)" font-size="14">LLM</text>
                </g>

                <g class="node n-user" transform="translate(100, 450)">
                    <rect width="200" height="100" rx="15" fill="rgba(0,0,0,0.5)" stroke="var(--border-color)"
                        stroke-width="2" />
                    <text x="100" y="52" fill="var(--text-primary)" text-anchor="middle" dominant-baseline="middle"
                        font-family="var(--font-sans)" font-size="18">User</text>
                </g>

                <!-- Animated Paths Data -->
                <!-- VIDEO to VisionAssist -->
                <path class="arrow p-vid-va" d="M 200 150 L 200 248 L 195 240 M 200 248 L 205 240"
                    stroke="var(--text-primary)" stroke-width="2" fill="none" stroke-linejoin="round" />

                <!-- User to VisionAssist -->
                <g class="arrow-group p-user-va">
                    <path class="arrow p-user-va-path" d="M 200 450 L 200 352 L 195 360 M 200 352 L 205 360"
                        stroke="var(--text-primary)" stroke-width="2" fill="none" stroke-linejoin="round" />
                    <text x="180" y="405" fill="var(--text-secondary)" text-anchor="end" font-family="var(--font-mono)"
                        font-size="14">asks</text>
                </g>

                <!-- VisionAssist to ML Model -->
                <path class="arrow p-va-ml" d="M 300 300 L 498 300 L 490 295 M 498 300 L 490 305"
                    stroke="var(--text-primary)" stroke-width="2" fill="none" stroke-linejoin="round" />

                <!-- ML Model to User (lines down, then left) -->
                <g class="arrow-group p-ml-user">
                    <path class="arrow p-ml-user-path" d="M 600 350 L 600 500 L 302 500 L 310 495 M 302 500 L 310 505"
                        stroke="var(--text-primary)" stroke-width="2" fill="none" stroke-linejoin="round" />
                    <text x="450" y="490" fill="var(--text-secondary)" text-anchor="middle"
                        font-family="var(--font-mono)" font-size="14">continuous feedback</text>
                </g>
            </svg>
        </div>

        <div class="features-grid">
            <div class="feature-card" data-tilt>
                <div class="feature-icon">SEGMENTATION</div>
                <h3>YOLOv11 Custom</h3>
                <p>Proprietary segmentation model trained on high-contrast urban navigation datasets. Extracts
                    pixel-perfect object boundaries instantly with TensorRT acceleration.</p>
            </div>
            <div class="feature-card" data-tilt>
                <div class="feature-icon">SEMANTICS</div>
                <h3>Vision-Language Synthesis</h3>
                <p>Multi-modal AI contextualizes the environment. It doesn't just see a "chair"; it understands "an
                    obstacle blocking the immediate path."</p>
            </div>
            <div class="feature-card" data-tilt>
                <div class="feature-icon">TELEMETRY</div>
                <h3>Spatial Haptics</h3>
                <p>Translates 3D spatial data into high-fidelity haptic waveforms. Feel the proximity, speed, and
                    dimensions of objects around you via wearable matrices.</p>
            </div>
        </div>

        <div class="architecture-deep-dive">
            <div class="deep-dive-block">
                <h3>Continuous Feedback Loop</h3>
                <p>At the center of Occulant's architecture is the real-time feedback loop. A stereo
                    <strong>VIDEO</strong> array feeds continuous optical data to our local
                    <strong>VisionAssist</strong> edge processor. This low-power module manages raw signal denoising
                    before handing it over to the cloud or local <strong>ML Model</strong>.</p>
            </div>
            <div class="deep-dive-block">
                <h3>Multi-Stage Perception</h3>
                <p>Unlike standard detection pipelines, our approach utilizes dual-stage validation. A lightweight
                    <strong>LLM</strong> runs asynchronously alongside the core detection engine, correcting semantic
                    misclassifications and providing a cohesive situational context (e.g., recognizing that a collection
                    of segmented metal parts is actually a bicycle).</p>
            </div>
            <div class="deep-dive-block">
                <h3>The Final Mile: Interaction</h3>
                <p>Data from the models must be synthesized predictably. Through spatial sonification and directed
                    haptics, the <strong>User</strong> node receives continuous feedback without sensory overload. The
                    user directly issues <strong>asks</strong> directly to VisionAssist (via voice or peripheral
                    triggers), instantly recalibrating the network's priority vectors.</p>
            </div>
        </div>
    </section>

    <section id="haptics" class="system-section haptics-section">
        <div class="section-header">
            <h2>Haptics & Segmentation</h2>
            <p>Real-time semantic segmentation directs our high-fidelity haptic feedback system, perfectly isolating
                dynamic obstacles.</p>
        </div>

        <div class="haptics-layout">
            <div class="haptics-text">
                <h3>Bridging Perception to Sensation</h3>
                <p>The visual world is hopelessly dense. Occulant extracts the salient features of any scene in
                    real-time, masking out irrelevant background noise and focusing computing power entirely on
                    navigational hazards.</p>
                <p>In the demonstrator adjacent, the <strong>YOLOv11 Segmentation</strong> module precisely masks a
                    pedestrian. The Vision-Language Model interprets this as an immediate trajectory threat and
                    formulates actionable language. This textual vector is instantly converted to a physical map: a
                    rapid pulsing sequence localized on the user's left haptic actuator array, alerting them to slow
                    down.</p>
                <p class="highlight-text">No screens. No delay. Pure intuition.</p>
            </div>

            <div class="haptics-demonstration">
                <div class="video-container">
                    <video src="haptics_video.mp4" autoplay loop muted playsinline class="segmentation-video"></video>

                    <div class="ui-overlay">
                        <div class="top-left corner"></div>
                        <div class="top-right corner"></div>
                        <div class="bottom-left corner"></div>
                        <div class="bottom-right corner"></div>

                        <div class="status-panel">
                            <div class="status-item">
                                <div class="dot blinking"></div> SEGMENTATION: ACTIVE
                            </div>
                            <div class="status-item">
                                <div class="dot blinking"></div> SPATIAL MAP: LINKED
                            </div>
                        </div>

                        <div class="terminal-readout context-box" id="haptics-context">
                            <span id="haptic-msg-1" style="opacity: 0; transition: opacity 0.5s ease;">> Keep heading
                                straight.</span><br>
                            <span id="haptic-msg-2"
                                style="opacity: 0; transition: opacity 0.5s ease; transition-delay: 2s;">> Slow down,
                                person ahead.</span>
                        </div>
                    </div>
                    <div class="sam-mask"></div>
                </div>
            </div>
        </div>
    </section>

    <section id="demo" class="system-section demo-section">
        <div class="section-header">
            <h2>Live Demo</h2>
            <p>Experience BlindAssit navigation assistance with real-time YOLO11 detection.</p>
        </div>

        <div class="demo-container">
            <div class="demo-instructions">
                <h3>üöÄ Quick Start</h3>
                <div class="step-list">
                    <div class="step">
                        <div class="step-number">1</div>
                        <div class="step-content">
                            <h4>Clone the Repository</h4>
                            <code>git clone https://github.com/manthanabc/BlindAssit.git</code>
                        </div>
                    </div>
                    <div class="step">
                        <div class="step-number">2</div>
                        <div class="step-content">
                            <h4>Install Dependencies</h4>
                            <code>cd BlindAssit && pip install -r requirements.txt</code>
                        </div>
                    </div>
                    <div class="step">
                        <div class="step-number">3</div>
                        <div class="step-content">
                            <h4>Set OpenRouter API Key (Optional)</h4>
                            <code>export OPENROUTER_API_KEY="your-key"</code>
                        </div>
                    </div>
                    <div class="step">
                        <div class="step-number">4</div>
                        <div class="step-content">
                            <h4>Start the Server</h4>
                            <code>./start.sh</code>
                        </div>
                    </div>
                    <div class="step">
                        <div class="step-number">5</div>
                        <div class="step-content">
                            <h4>Open in Browser</h4>
                            <code>http://localhost:5000</code>
                        </div>
                    </div>
                </div>

                <div class="demo-features">
                    <h3>‚ú® Features</h3>
                    <ul>
                        <li>üé• Real-time camera feed</li>
                        <li>üö∂ Sidewalk/obstacle detection</li>
                        <li>üîä Voice navigation guidance</li>
                        <li>ü§ñ OpenRouter AI queries</li>
                        <li>üì≥ Vibration API feedback</li>
                        <li>üåô Dark monochrome UI</li>
                    </ul>
                </div>

                <div class="tech-stack">
                    <h3>üõ†Ô∏è Tech Stack</h3>
                    <p>Flask + YOLO11 + PyTorch + OpenCV + gTTS + OpenRouter AI + JavaScript + Vibration API</p>
                </div>
            </div>

            <div class="demo-video">
                <div class="video-wrapper">
                    <video src="haptics_video.mp4" controls class="demo-video-player">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-caption">
                        <p>BlindAssit in action - Real-time sidewalk navigation</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="footer-content">
            <div class="footer-logo">OCCULANT<span>.sys</span></div>
            <p class="copyright">&copy; 2026 Occulant Vision Systems. All rights reserved.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>